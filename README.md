# ğŸš€ AI Production Support Assistant (Ops Copilot)

An enterprise-style **L2 Production Support Assistant** powered by RAG, LangChain, and OpenAI.
The system helps operations teams quickly diagnose incidents, follow runbooks, and investigate alerts/tickets using natural language.

---

## ğŸ§­ Overview

Modern production environments generate large volumes of runbooks, alerts, incidents, logs, and tickets. During outages, L2 engineers often lose time searching across multiple systems.

This project demonstrates an **Ops Copilot** that:

* understands support queries
* routes them intelligently
* retrieves the most relevant operational knowledge
* returns structured, actionable steps

The design mirrors real-world SRE / production support workflows in banking and trading environments.

---

## âœ¨ Key Features

* ğŸ” **Multi-source RAG** over runbooks, incidents, alerts, logs, and tickets
* ğŸ§  **Intent-aware routing** for precise retrieval
* ğŸ“Š **Structured L2 responses** (steps, service, escalation, confidence)
* ğŸ›¡ï¸ **Guardrails** for out-of-scope questions
* ğŸ’¬ **Chat-style Flask UI**
* ğŸ“ˆ **LangSmith tracing** for observability
* ğŸ§ª **Evaluation harness** with LLM judge
* ğŸ¦ Realistic **banking/trade management scenarios**

---

## ğŸ—ï¸ Architecture

**High-level flow**

User Query â†’ Intent Classifier â†’ Source Routing â†’ Vector Retrieval â†’ RAG Generation â†’ Structured Output â†’ UI

**Core capabilities**

* Semantic search using FAISS
* Metadata-aware filtering
* Prompt-engineered intent classification
* Deterministic reference attribution
* Production-style confidence scoring

---

## ğŸ“ Project Structure

```
with_langchain/
â”‚
â”œâ”€â”€ ingestion/          # data loading and indexing
â”œâ”€â”€ retrieval/          # retrievers, routing, filters
â”œâ”€â”€ chains/             # intent + RAG chains
â”œâ”€â”€ core/               # config, prompts, models
â”œâ”€â”€ ui/                 # Flask web interface
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ templates/
â”‚   â””â”€â”€ static/
â”œâ”€â”€ vector_store/       # FAISS index (generated)
â”œâ”€â”€ evaluation/         # eval datasets and runner
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start (Local)

### 1ï¸âƒ£ Clone the repository

```bash
git clone <your-repo-url>
cd with_langchain
```

---

### 2ï¸âƒ£ Create and activate virtual environment

**Windows**

```bash
python -m venv venv
venv\Scripts\activate
```

**Mac/Linux**

```bash
python -m venv venv
source venv/bin/activate
```

---

### 3ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

---

### 4ï¸âƒ£ Configure environment variables

Create a `.env` file in the project root:

```env
OPENAI_API_KEY=your_openai_key
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=your_langsmith_key
LANGCHAIN_PROJECT=ops-support-assistant
```

> âš ï¸ Never commit `.env` to GitHub.

---

### 5ï¸âƒ£ Build the vector index (first time only)

```bash
python -m ingestion.build_index
```

---

### 6ï¸âƒ£ Run the application

```bash
python -m ui.app
```

Open in browser:

```
http://localhost:5000
```

---

## ğŸ§ª Example Queries

Try the following in the UI:

**Runbook**

```
disk usage high on trade server
```

**Alert**

```
critical alert trade settlement queue depth high
```

**Ticket**

```
SR-Trade-003 users reporting payment timeouts
```

**Incident**

```
why did eod reconciliation fail
```

**Guardrail**

```
what is the weather today
```

---

## ğŸ“Š Structured Response Format

The assistant returns:

* Issue Summary
* Impacted Service
* Recommended Steps
* Escalation Required
* Confidence Level
* Reference Documents

This mirrors real L2 support workflows.

---

## ğŸ” Evaluation

An evaluation harness is included to measure:

* intent accuracy
* retrieval quality
* answer correctness
* confidence calibration

Run evaluation:

```bash
python evaluation/eval_runner.py
```

---

## ğŸ§  Observability (LangSmith)

Tracing is enabled via LangSmith for:

* prompt inspection
* retrieval debugging
* latency tracking
* token usage

Configure via environment variables.

---

## ğŸŒ Deployment

The application is designed to deploy easily on:

* Render (recommended for demo)
* Railway
* Docker + Cloud (advanced)

**Start command for production**

```bash
gunicorn ui.app:app
```

---

## ğŸ›¡ï¸ Guardrails

The assistant intentionally refuses:

* weather queries
* personal questions
* non-ops chit-chat

This prevents hallucinations and keeps the system production-focused.

---

## ğŸ”® Future Enhancements

* Cross-encoder reranking
* Streaming responses
* RBAC / authentication
* Real log ingestion pipeline
* Kubernetes deployment
* FastAPI migration

---

## ğŸ‘©â€ğŸ’» Author

**Pallavi**
L2 Production Support Specialist | GenAI Engineer

This project demonstrates how GenAI can augment real-world production support operations in banking/trading environments.

---

## ğŸ“œ License

This project is for educational and demonstration purposes.
